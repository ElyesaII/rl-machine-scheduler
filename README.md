# Q-Learning pour l'Ordonnancement de Production (Job Shop Scheduling)

## ğŸ“‹ Description du Projet

Ce projet implÃ©mente un systÃ¨me d'apprentissage par renforcement (Q-Learning) pour rÃ©soudre le problÃ¨me d'ordonnancement de production (Job Shop Scheduling Problem - JSSP). L'objectif est d'optimiser l'affectation d'opÃ©rations de production sur des machines afin de minimiser le temps total de fabrication (makespan).

### ProblÃ©matique

Le Job Shop Scheduling consiste Ã  :
- Planifier **N produits** sur **M machines**
- Chaque produit nÃ©cessite une sÃ©quence d'opÃ©rations dans un ordre spÃ©cifique
- Chaque opÃ©ration doit Ãªtre rÃ©alisÃ©e sur une machine donnÃ©e et prend un temps dÃ©fini
- Les contraintes incluent :
  - Une machine ne peut traiter qu'une opÃ©ration Ã  la fois
  - Les opÃ©rations d'un produit doivent respecter leur ordre de sÃ©quence
  - Minimiser le temps total (makespan)

## ğŸ§  ImplÃ©mentation Q-Learning

### Composants de l'Algorithme

#### **Ã‰tat (State)**
L'Ã©tat du systÃ¨me est dÃ©fini par :
- La progression de chaque produit (quelle opÃ©ration est en cours)
- L'Ã©tat de disponibilitÃ© de chaque machine
- Le temps actuel dans le planning

#### **Actions**
Les actions possibles sont :
- Affecter la prochaine opÃ©ration disponible d'un produit Ã  une machine libre

#### **RÃ©compenses (Rewards)**
Le systÃ¨me de rÃ©compense est conÃ§u pour :
- **RÃ©compense nÃ©gative proportionnelle au temps** : -0.1 Ã— durÃ©e de l'opÃ©ration
- **RÃ©compense positive pour la complÃ©tion** : +1000 / makespan final
- **Bonus d'utilisation** : +10 Ã— taux d'utilisation des machines
- **PÃ©nalitÃ© forte** : pour les violations de contraintes

#### **ParamÃ¨tres Q-Learning**
- **Î± (learning rate)** : Taux d'apprentissage (dÃ©faut: 0.1)
  - ContrÃ´le la vitesse d'apprentissage
  - Valeurs Ã©levÃ©es = apprentissage rapide mais instable
  
- **Î³ (discount factor)** : Facteur d'actualisation (dÃ©faut: 0.95)
  - Importance des rÃ©compenses futures
  - Proche de 1 = vision Ã  long terme
  
- **Îµ (epsilon)** : Taux d'exploration (dÃ©faut: 0.3)
  - Ã‰quilibre exploration/exploitation
  - 0.3 = 30% d'actions alÃ©atoires pour explorer

### Ã‰quation de Mise Ã  Jour

```
Q(s,a) â† Q(s,a) + Î±[r + Î³ max Q(s',a') - Q(s,a)]
                              a'
```

OÃ¹ :
- `s` = Ã©tat actuel
- `a` = action choisie
- `r` = rÃ©compense obtenue
- `s'` = nouvel Ã©tat
- `a'` = actions possibles depuis le nouvel Ã©tat

## ğŸ¯ FonctionnalitÃ©s

### Configuration du ProblÃ¨me
- DÃ©finir le nombre de machines (1-10)
- CrÃ©er des produits avec des sÃ©quences d'opÃ©rations personnalisÃ©es
- SpÃ©cifier pour chaque opÃ©ration :
  - La machine requise
  - La durÃ©e de traitement

### Modes d'Apprentissage
1. **Mode Pas-Ã -Pas** : ExÃ©cuter un Ã©pisode d'apprentissage Ã  la fois pour observer le processus
2. **Mode Automatique** : EntraÃ®nement continu pour convergence rapide

### Visualisations

#### 1. Diagramme de Gantt
- ReprÃ©sentation visuelle du planning optimal
- Chaque couleur reprÃ©sente un produit diffÃ©rent
- Axe horizontal = temps
- Axe vertical = machines
- Affiche le makespan actuel

#### 2. Visualisation de la Q-Table
- Top 50 des paires Ã©tat-action
- Couleur verte = valeur Q positive (bonne action)
- Couleur rouge = valeur Q nÃ©gative (mauvaise action)
- IntensitÃ© = magnitude de la valeur

#### 3. Statistiques en Temps RÃ©el
- Nombre d'Ã©pisodes d'entraÃ®nement
- RÃ©compense totale de l'Ã©pisode en cours
- Meilleur makespan trouvÃ©

## ğŸš€ Installation et Utilisation

### PrÃ©requis
- Node.js >= 18.0.0
- npm ou yarn

### Installation des DÃ©pendances

```bash
npm install
```

DÃ©pendances principales :
- **React 18** : Framework UI
- **TypeScript** : Typage statique
- **Vite** : Build tool rapide
- **Tailwind CSS** : Styling
- **shadcn/ui** : Composants UI
- **Lucide React** : IcÃ´nes
- **Sonner** : Notifications toast

### Lancement de l'Application

```bash
npm run dev
```

L'application sera accessible sur `http://localhost:8080`

### Build Production

```bash
npm run build
```

## ğŸ“Š Utilisation de l'Interface

### 1. Configuration Initiale
1. DÃ©finissez le nombre de machines
2. Ajoutez des produits avec le bouton "Ajouter Produit"
3. Pour chaque produit, dÃ©finissez ses opÃ©rations :
   - NumÃ©ro de machine (0 Ã  N-1)
   - DurÃ©e de l'opÃ©ration
4. Cliquez sur "DÃ©marrer l'Apprentissage"

### 2. EntraÃ®nement du ModÃ¨le
1. **Un Ã‰pisode** : Lance un cycle complet d'apprentissage
   - L'agent essaie de planifier tous les produits
   - La Q-table est mise Ã  jour
   - Le meilleur planning est affichÃ© si trouvÃ©

2. **Mode Auto** : Lance l'entraÃ®nement en continu
   - 10 Ã©pisodes par seconde
   - ArrÃªt avec le bouton "Stop"
   - Observe l'amÃ©lioration progressive du makespan

### 3. Ajustement des HyperparamÃ¨tres
- Modifiez Î±, Î³, et Îµ selon vos besoins
- Valeurs recommandÃ©es :
  - Î± = 0.1-0.3 pour stabilitÃ©
  - Î³ = 0.9-0.99 pour vision long terme
  - Îµ = 0.2-0.4 pour bon Ã©quilibre exploration/exploitation

### 4. Analyse des RÃ©sultats
- **Gantt Chart** : Visualisez le planning optimal trouvÃ©
- **Q-Table** : Comprenez quelles actions sont favorisÃ©es
- **Statistiques** : Suivez la progression de l'apprentissage

## ğŸ”¬ Exemple de ProblÃ¨me

### Configuration Simple (2 machines, 2 produits)

**Produit 1:**
- OpÃ©ration 1: Machine 0, DurÃ©e 5
- OpÃ©ration 2: Machine 1, DurÃ©e 3

**Produit 2:**
- OpÃ©ration 1: Machine 1, DurÃ©e 4
- OpÃ©ration 2: Machine 0, DurÃ©e 2

**Solution Optimale Attendue:**
- Makespan â‰ˆ 9-10 unitÃ©s
- ParallÃ©lisation des opÃ©rations quand possible

## ğŸ“š RÃ©fÃ©rences ThÃ©oriques

### Algorithme Q-Learning
- Watkins, C.J.C.H. (1989). "Learning from Delayed Rewards"
- Sutton & Barto (2018). "Reinforcement Learning: An Introduction"

### Job Shop Scheduling
- Brucker, P. (2007). "Scheduling Algorithms"
- Pinedo, M. (2012). "Scheduling: Theory, Algorithms, and Systems"

## ğŸ› ï¸ Architecture du Code

```
src/
â”œâ”€â”€ components/
â”‚   â”œâ”€â”€ ProblemSetup.tsx          # Configuration du problÃ¨me
â”‚   â”œâ”€â”€ TrainingControls.tsx      # ContrÃ´les d'apprentissage
â”‚   â”œâ”€â”€ GanttChart.tsx            # Visualisation du planning
â”‚   â”œâ”€â”€ QTableVisualization.tsx   # Visualisation Q-table
â”‚   â””â”€â”€ ui/                       # Composants UI rÃ©utilisables
â”œâ”€â”€ lib/
â”‚   â””â”€â”€ qlearning.ts              # ImplÃ©mentation Q-Learning
â”œâ”€â”€ pages/
â”‚   â””â”€â”€ Index.tsx                 # Page principale
â””â”€â”€ index.css                     # Design system
```

### Fichier Principal: `qlearning.ts`

Contient :
- **`QLearningScheduler`** : Classe principale
  - `trainEpisode()` : ExÃ©cute un Ã©pisode d'apprentissage
  - `getBestSchedule()` : Obtient le meilleur planning (exploitation pure)
  - `getQTableEntries()` : RÃ©cupÃ¨re les entrÃ©es de la Q-table
  - MÃ©thodes privÃ©es pour l'encodage Ã©tat/action, calcul de rÃ©compense, etc.

## ğŸ“ Concepts PÃ©dagogiques

Ce projet illustre :
1. **Processus de DÃ©cision Markovien (MDP)** : Ã‰tats, actions, transitions, rÃ©compenses
2. **Ã‰quilibre Exploration/Exploitation** : StratÃ©gie epsilon-greedy
3. **Apprentissage par diffÃ©rence temporelle** : Mise Ã  jour incrÃ©mentale des valeurs Q
4. **Optimisation combinatoire** : Application pratique du RL Ã  un problÃ¨me NP-difficile

## âš™ï¸ Extensions Possibles

- ImplÃ©menter Deep Q-Learning (DQN) pour des problÃ¨mes plus complexes
- Ajouter des contraintes supplÃ©mentaires (deadlines, dÃ©pendances)
- Optimisation multi-objectifs (temps + coÃ»t + Ã©nergie)
- Comparaison avec des heuristiques classiques (SPT, LPT, etc.)
- Export/Import de configurations de problÃ¨mes

## ğŸ“„ Licence

Projet Ã©ducatif - INP-Ensiacet 3A IMSIC

## ğŸ‘¨â€ğŸ’» Auteur

DÃ©veloppÃ© pour le cours d'Apprentissage par Renforcement 2025-2026
